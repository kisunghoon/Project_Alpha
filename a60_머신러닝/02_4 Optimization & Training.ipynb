{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim      # Optimizer\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1   # shuffle 시 동일하게 섞기\n",
    "# DataLoader() 에서 shuffle=True 을 주어 데이터를 섞을텐데\n",
    "# 매번 다르게 섞는게 아니라 동일하게 (seed) 섞을수 있도록 함.\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "no_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = not no_cuda and torch.cuda.is_available()   # cuda 사용여부\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r'D:\\DevRoot\\DataSet\\torch\\dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)   # shuffle 시 매번 동일하게 shuffle됨 \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        save_dir, train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        save_dir, train=False, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    ),\n",
    "    batch_size = test_batch_size,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)  # model 불러와서 device 에 담아줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 0.1031, -0.0883, -0.0388,  0.0939, -0.1883],\n",
       "           [ 0.1199, -0.0411,  0.1017,  0.0278, -0.0245],\n",
       "           [ 0.0555,  0.0099,  0.0730, -0.0779, -0.0146],\n",
       "           [-0.0180,  0.0290, -0.0008,  0.1748,  0.0622],\n",
       "           [-0.0745, -0.1208, -0.0335, -0.0863, -0.0641]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0096,  0.1192,  0.1087, -0.1955,  0.1240],\n",
       "           [ 0.0559,  0.1897,  0.1320, -0.1822, -0.1902],\n",
       "           [-0.0965,  0.1756, -0.0333,  0.0856, -0.0929],\n",
       "           [ 0.1962, -0.0846,  0.1500,  0.0024, -0.1054],\n",
       "           [ 0.1028, -0.1062,  0.0588, -0.0578, -0.0219]]],\n",
       " \n",
       " \n",
       "         [[[-0.1923, -0.0954,  0.1085, -0.0486,  0.1992],\n",
       "           [ 0.1603, -0.0094, -0.1335,  0.1218,  0.0621],\n",
       "           [-0.1293,  0.1299,  0.1214,  0.1774, -0.1121],\n",
       "           [-0.0329, -0.0039,  0.0292, -0.1518, -0.1419],\n",
       "           [ 0.1088, -0.0469,  0.0977,  0.0114,  0.0657]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0440,  0.0727,  0.0991, -0.1852,  0.1007],\n",
       "           [-0.1406, -0.1509,  0.0122, -0.0341,  0.1175],\n",
       "           [-0.1158, -0.1778,  0.1456, -0.0297,  0.1125],\n",
       "           [ 0.0643, -0.1500,  0.0402,  0.0480, -0.1339],\n",
       "           [-0.0949,  0.0682,  0.0358, -0.0851, -0.0605]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1832, -0.0370,  0.1128,  0.0866, -0.1293],\n",
       "           [-0.1701,  0.1920,  0.0104,  0.1371,  0.0414],\n",
       "           [ 0.0643,  0.1494,  0.1896, -0.1327,  0.0250],\n",
       "           [ 0.1493,  0.1449,  0.1242, -0.1447, -0.1440],\n",
       "           [-0.1210,  0.0251,  0.1993, -0.1263,  0.1066]]],\n",
       " \n",
       " \n",
       "         [[[-0.1107, -0.1880, -0.0425,  0.1152,  0.1857],\n",
       "           [-0.1242,  0.0434,  0.1726,  0.1325,  0.1246],\n",
       "           [ 0.1421,  0.1265,  0.0517, -0.1368, -0.1680],\n",
       "           [-0.0916, -0.0233, -0.1226,  0.0732,  0.0619],\n",
       "           [-0.0453,  0.0769,  0.0646,  0.1221,  0.1347]]],\n",
       " \n",
       " \n",
       "         [[[-0.0677,  0.1954, -0.0231, -0.0069, -0.1888],\n",
       "           [-0.1287, -0.1168, -0.0855,  0.1422, -0.0654],\n",
       "           [-0.1495,  0.0769,  0.0640,  0.1295, -0.1035],\n",
       "           [ 0.0434, -0.0728, -0.0449, -0.1594, -0.0912],\n",
       "           [-0.0613,  0.0855,  0.0365,  0.0494,  0.1996]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1949,  0.1364,  0.0064, -0.1384,  0.1563],\n",
       "           [-0.0500, -0.0162, -0.1723, -0.0395, -0.1290],\n",
       "           [ 0.1838, -0.1729, -0.1559, -0.0068, -0.1081],\n",
       "           [ 0.0716, -0.0770, -0.0939,  0.0113,  0.1448],\n",
       "           [-0.1407,  0.0939,  0.1285,  0.1956, -0.1400]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0484, -0.1479,  0.1708, -0.0776,  0.1205],\n",
       "           [ 0.0060, -0.0156, -0.0064,  0.0340,  0.0943],\n",
       "           [ 0.0321,  0.0610, -0.1799,  0.1457,  0.1744],\n",
       "           [ 0.1653,  0.1478, -0.1443, -0.0741,  0.1763],\n",
       "           [-0.1523,  0.1814, -0.1573, -0.1409,  0.0978]]],\n",
       " \n",
       " \n",
       "         [[[-0.1437, -0.0458,  0.1455,  0.1584,  0.1892],\n",
       "           [-0.0406, -0.1555,  0.1969, -0.0426, -0.0823],\n",
       "           [ 0.0488, -0.1399,  0.1315,  0.1253, -0.1587],\n",
       "           [-0.1643, -0.0175,  0.0840, -0.0058, -0.1014],\n",
       "           [ 0.0046, -0.1880, -0.1414, -0.1331,  0.1647]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1763, -0.0679,  0.0090,  0.0892,  0.0240],\n",
       "           [-0.1002,  0.1153,  0.1229, -0.0116, -0.0246],\n",
       "           [ 0.1817,  0.1748, -0.1134,  0.1957,  0.0495],\n",
       "           [-0.1329,  0.1095, -0.1493,  0.1848, -0.1286],\n",
       "           [ 0.0566,  0.0609,  0.0476,  0.1659, -0.0831]]],\n",
       " \n",
       " \n",
       "         [[[-0.0844, -0.1733, -0.0082, -0.0947,  0.0080],\n",
       "           [-0.0410,  0.0664,  0.1730,  0.0590, -0.0644],\n",
       "           [-0.0981, -0.1744,  0.1683, -0.0379,  0.0404],\n",
       "           [ 0.0074, -0.1275,  0.1126,  0.1121,  0.0101],\n",
       "           [-0.1137, -0.0850,  0.0028, -0.1143, -0.1116]]],\n",
       " \n",
       " \n",
       "         [[[-0.0298, -0.0576,  0.0491, -0.0526, -0.0265],\n",
       "           [-0.0763, -0.1828,  0.1744,  0.0382,  0.1788],\n",
       "           [-0.0322, -0.0503, -0.1880, -0.0759,  0.1312],\n",
       "           [-0.1959, -0.0700,  0.1219,  0.1153,  0.0738],\n",
       "           [-0.0396, -0.0151, -0.0380, -0.0737,  0.0835]]],\n",
       " \n",
       " \n",
       "         [[[-0.0681, -0.1124,  0.0454, -0.1446, -0.0427],\n",
       "           [ 0.1022,  0.0747,  0.1703, -0.1364,  0.0017],\n",
       "           [-0.0591, -0.1119, -0.1125,  0.0504,  0.0209],\n",
       "           [-0.1021, -0.1410, -0.1657,  0.1960, -0.0341],\n",
       "           [-0.0462, -0.0735, -0.1000, -0.1829, -0.1174]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1222,  0.0438, -0.0706, -0.0756,  0.1278],\n",
       "           [ 0.1442,  0.1927,  0.0583,  0.0966, -0.0161],\n",
       "           [-0.1169, -0.1959,  0.1218, -0.0291,  0.0822],\n",
       "           [ 0.0098, -0.1867, -0.0981,  0.0563, -0.0405],\n",
       "           [-0.1931,  0.0701,  0.0077, -0.0643,  0.0808]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0422, -0.0738,  0.1797,  0.1003,  0.0251],\n",
       "           [-0.0384, -0.1694,  0.1544, -0.0516, -0.1292],\n",
       "           [ 0.1282,  0.0710,  0.0495, -0.0397, -0.0624],\n",
       "           [ 0.0339,  0.1548,  0.0286, -0.0946,  0.1987],\n",
       "           [ 0.0043,  0.1581,  0.0487,  0.1793,  0.1154]]],\n",
       " \n",
       " \n",
       "         [[[-0.1020,  0.1405,  0.0013, -0.1605, -0.0419],\n",
       "           [-0.1805, -0.1036, -0.1417,  0.1368, -0.0420],\n",
       "           [-0.0346,  0.0958,  0.0814, -0.1320,  0.0737],\n",
       "           [ 0.0850, -0.1590,  0.1739, -0.1217, -0.0782],\n",
       "           [-0.1647,  0.1828,  0.0381,  0.1684,  0.0131]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1087, -0.0639, -0.1596,  0.0321, -0.0178],\n",
       "           [-0.0202, -0.0982,  0.0041,  0.0574,  0.0721],\n",
       "           [-0.1611,  0.0966,  0.0206,  0.1379,  0.1413],\n",
       "           [-0.1575,  0.1921, -0.1967,  0.1150,  0.0141],\n",
       "           [ 0.0946, -0.1082,  0.1202, -0.0990, -0.1768]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0670,  0.1095,  0.1983, -0.0209,  0.1924],\n",
       "           [ 0.1285, -0.0992, -0.1543,  0.1122, -0.1096],\n",
       "           [ 0.0917, -0.1713,  0.1459,  0.0599, -0.0230],\n",
       "           [ 0.0917, -0.1424, -0.0437,  0.0020, -0.1156],\n",
       "           [-0.0511, -0.0863,  0.1906,  0.1699,  0.1725]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1635, -0.1915,  0.0669,  0.0737, -0.1467],\n",
       "           [ 0.0811, -0.1085,  0.0318,  0.0662,  0.1726],\n",
       "           [-0.1049, -0.1574,  0.1851,  0.0840,  0.0972],\n",
       "           [ 0.0720, -0.0708,  0.1203, -0.1746,  0.0278],\n",
       "           [ 0.0904,  0.1366, -0.0999, -0.1352,  0.0750]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1588, -0.1662, -0.1799, -0.0970, -0.0382, -0.1184, -0.0846, -0.1771,\n",
       "          0.1229, -0.0458, -0.1262,  0.0521, -0.1495,  0.1400,  0.1150, -0.1965,\n",
       "         -0.1570, -0.1319, -0.1083,  0.0250], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-4.4223e-02, -3.3155e-02,  2.0424e-02, -1.4647e-02, -1.7796e-02],\n",
       "           [-3.6997e-02,  1.8126e-02,  3.1580e-03, -1.7286e-02, -2.7387e-02],\n",
       "           [ 9.5110e-03,  1.2168e-02,  1.9850e-02,  1.3290e-02, -6.8220e-03],\n",
       "           [-3.4619e-02,  2.8470e-02, -3.8335e-04, -2.9721e-02,  3.8567e-02],\n",
       "           [ 2.4934e-02, -3.5148e-02, -2.2335e-02, -1.5219e-02, -2.5817e-02]],\n",
       " \n",
       "          [[ 2.0884e-02, -3.1904e-02,  4.1563e-02, -1.8485e-02,  2.6398e-02],\n",
       "           [ 1.5230e-03, -1.9668e-02,  2.9864e-02, -3.4124e-02, -2.3661e-02],\n",
       "           [ 5.3603e-03,  3.5476e-02, -1.9161e-02, -2.7232e-02, -2.8551e-02],\n",
       "           [-1.9717e-02, -1.5448e-02, -1.0417e-02, -2.5441e-02,  1.3977e-02],\n",
       "           [ 3.6320e-04, -2.9220e-02, -2.5539e-02,  9.4698e-03, -6.3886e-04]],\n",
       " \n",
       "          [[ 3.1651e-02, -6.7796e-03, -3.6229e-02, -3.3076e-02, -1.3127e-02],\n",
       "           [-9.9048e-03,  5.1037e-03, -1.0030e-02,  1.6501e-02, -1.4874e-02],\n",
       "           [ 3.2823e-02,  1.0272e-02, -1.3749e-02,  3.9856e-02,  1.1417e-02],\n",
       "           [-3.0392e-02, -2.4889e-02,  2.8041e-02,  2.0425e-02,  3.4918e-02],\n",
       "           [-3.4353e-03,  3.1279e-02,  2.2234e-02,  1.3830e-02, -1.0351e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9528e-03, -3.1579e-02,  2.0421e-03, -1.2975e-02,  1.5787e-02],\n",
       "           [ 3.7829e-02, -2.4478e-02,  1.8727e-03,  4.3498e-02, -3.5701e-02],\n",
       "           [-7.7173e-03,  4.3427e-02,  3.1248e-02,  1.2980e-02, -1.9288e-02],\n",
       "           [-4.1689e-03, -4.0103e-02, -4.2104e-02, -4.1901e-02,  4.2954e-02],\n",
       "           [ 3.5371e-02,  1.7740e-02, -1.1233e-02,  3.8505e-02, -1.1813e-02]],\n",
       " \n",
       "          [[ 3.1047e-02, -1.4555e-03, -2.1037e-02, -1.2482e-02, -9.5493e-03],\n",
       "           [-4.0935e-02,  1.3373e-02, -1.3626e-02,  2.0547e-02,  2.8514e-02],\n",
       "           [-7.1102e-03,  2.1689e-02,  8.9351e-03,  3.0033e-02, -2.8841e-02],\n",
       "           [-3.9263e-03, -1.4902e-02,  3.9186e-02, -4.0142e-02, -2.5137e-02],\n",
       "           [ 5.0486e-03,  3.9701e-02,  3.0259e-02, -4.1848e-02, -3.1442e-02]],\n",
       " \n",
       "          [[ 4.3362e-02,  6.5175e-03, -3.8304e-02, -1.4089e-02,  1.9552e-02],\n",
       "           [-9.3516e-03, -3.4352e-02, -2.7612e-02, -2.3167e-02, -1.7517e-03],\n",
       "           [ 1.7526e-02,  2.1472e-02, -2.1725e-02, -7.5036e-03, -3.4529e-02],\n",
       "           [ 3.3820e-02,  4.4630e-02,  4.3468e-02,  1.8277e-02, -9.5363e-03],\n",
       "           [-1.1148e-03,  1.6135e-02, -3.8765e-02,  2.7762e-03, -3.6352e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.4714e-02, -1.4947e-03, -1.4237e-02, -2.8329e-02,  7.4136e-03],\n",
       "           [ 1.8218e-03,  6.8245e-03, -2.5030e-02, -4.3804e-02,  9.7875e-04],\n",
       "           [-1.8244e-02, -1.5077e-02,  3.2923e-02,  7.3038e-03,  2.0425e-02],\n",
       "           [-1.2498e-02, -3.5486e-02,  8.9173e-03, -2.9617e-02,  5.5411e-03],\n",
       "           [-3.4902e-02,  3.3715e-02, -3.0465e-02,  3.8825e-02,  3.1591e-02]],\n",
       " \n",
       "          [[ 3.3807e-02,  2.6835e-02,  1.0351e-02, -2.1797e-03, -7.1386e-04],\n",
       "           [-1.7787e-02, -4.1108e-02,  4.0762e-02,  2.3171e-03,  2.1033e-02],\n",
       "           [ 1.6414e-03,  1.2852e-02, -2.7554e-02, -3.4765e-02,  3.4106e-02],\n",
       "           [ 1.6959e-02,  2.2082e-02, -4.4388e-02,  3.0235e-03, -4.1714e-02],\n",
       "           [-5.1178e-03, -2.3642e-03, -3.1462e-02, -4.4284e-02,  2.9434e-04]],\n",
       " \n",
       "          [[ 4.0218e-02,  4.0304e-02, -2.7227e-02, -7.9444e-03, -3.7398e-02],\n",
       "           [-3.4925e-02, -1.2262e-02,  4.3418e-02, -1.1667e-02,  2.5875e-03],\n",
       "           [ 4.2072e-02,  7.9697e-03,  3.6077e-02,  2.9717e-02,  4.1936e-02],\n",
       "           [ 7.6693e-03, -3.6165e-02, -3.8472e-02, -2.3107e-02,  3.6912e-02],\n",
       "           [ 2.5716e-02,  1.3089e-02, -3.4057e-02, -3.2937e-02,  3.6486e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.1548e-02, -1.8399e-02,  1.7712e-02, -2.6533e-02, -1.3825e-02],\n",
       "           [ 2.0035e-04,  2.6931e-02,  2.5255e-02, -3.9948e-03, -1.2649e-02],\n",
       "           [-7.7558e-04,  2.3049e-02, -3.7904e-02, -1.3665e-02, -4.3111e-02],\n",
       "           [ 6.4553e-03, -5.2898e-03,  2.5334e-02,  3.5727e-02, -3.8238e-02],\n",
       "           [-1.9792e-02, -7.1894e-03, -4.4595e-04, -1.0208e-02, -2.8118e-02]],\n",
       " \n",
       "          [[-1.7280e-02, -1.1517e-03,  3.5964e-02, -1.1397e-02,  2.9534e-02],\n",
       "           [-3.6151e-02,  1.9999e-03, -3.4899e-02, -4.1965e-02,  4.4320e-03],\n",
       "           [-2.0477e-02,  3.2924e-02,  3.5324e-02, -4.4512e-02, -3.1167e-03],\n",
       "           [-7.6401e-03, -2.5490e-02, -3.1222e-02, -1.8925e-02,  1.9745e-02],\n",
       "           [ 1.0633e-02,  2.6916e-02, -2.6672e-03,  2.3227e-02,  8.6481e-03]],\n",
       " \n",
       "          [[-9.7518e-03, -2.6144e-03, -4.0674e-02, -1.0270e-03, -1.4692e-02],\n",
       "           [-1.4966e-02, -1.7962e-02, -9.7776e-03, -3.3017e-02,  2.6243e-03],\n",
       "           [-1.8175e-03, -1.1779e-02,  3.8821e-02,  3.0013e-02,  1.2630e-02],\n",
       "           [-1.6472e-02,  5.7040e-03, -3.0771e-02,  3.1161e-02,  4.1771e-02],\n",
       "           [-6.6771e-03,  3.4489e-02,  4.1971e-02, -2.6871e-03, -1.6606e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2570e-03,  7.3905e-03, -4.1397e-02, -2.7898e-03, -1.6170e-02],\n",
       "           [ 2.9842e-02, -1.2838e-03,  2.9424e-02,  3.9779e-02, -1.4015e-02],\n",
       "           [ 9.6264e-03, -3.8256e-03, -1.9490e-02, -2.4808e-02,  1.0348e-03],\n",
       "           [ 1.3588e-02, -7.8656e-03,  7.5947e-03,  2.7947e-02,  1.8644e-02],\n",
       "           [ 1.8442e-02,  1.2568e-02, -3.4378e-02, -2.7057e-02, -3.3164e-02]],\n",
       " \n",
       "          [[ 2.9388e-02, -3.9389e-02, -3.3388e-02,  2.9452e-02,  2.6435e-02],\n",
       "           [-4.2276e-02,  2.5841e-02, -1.0204e-02,  9.6350e-03, -2.8015e-02],\n",
       "           [-6.7887e-03,  9.9396e-03,  3.4100e-02,  2.7119e-02, -2.4319e-02],\n",
       "           [-2.9334e-02, -2.7215e-03,  1.4035e-02,  2.4521e-02, -4.4565e-02],\n",
       "           [-3.6545e-03,  1.5815e-03, -2.0886e-02,  3.5273e-02, -1.1871e-02]],\n",
       " \n",
       "          [[-4.2637e-02, -2.6702e-02,  1.4339e-03, -3.3280e-02,  1.0215e-02],\n",
       "           [ 4.3505e-02,  1.8307e-03, -1.1017e-02,  4.2132e-02,  3.3015e-02],\n",
       "           [ 2.9718e-02,  3.3923e-03,  2.0445e-03, -3.4683e-02,  4.5524e-03],\n",
       "           [ 4.3498e-02,  2.3642e-03, -3.9909e-02,  2.4546e-02,  4.0136e-02],\n",
       "           [ 3.7492e-02,  2.1093e-03,  8.4015e-03,  7.2373e-03, -1.8329e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6370e-03,  1.8609e-03, -3.8643e-02,  3.7291e-02, -1.7092e-02],\n",
       "           [-3.6199e-02, -7.0716e-04, -3.9309e-02,  5.5800e-03, -2.8939e-02],\n",
       "           [-9.8076e-03, -1.3153e-02,  3.2363e-02,  3.4513e-02,  4.4334e-02],\n",
       "           [ 4.9264e-03,  2.8526e-02, -3.6701e-02, -2.9999e-02,  2.4068e-02],\n",
       "           [ 9.8239e-03,  1.2378e-02,  2.5565e-02,  2.8356e-02,  2.1017e-02]],\n",
       " \n",
       "          [[-1.6567e-02, -1.4166e-02,  9.4186e-03,  3.6806e-02, -3.3887e-03],\n",
       "           [-1.3312e-02,  2.4708e-02,  2.9648e-02, -8.3015e-04, -1.3225e-02],\n",
       "           [-4.4554e-02,  2.7476e-03,  1.3709e-02, -3.2833e-02, -3.3203e-02],\n",
       "           [-4.4710e-02,  3.4944e-02,  4.1924e-02,  4.4259e-02,  3.9925e-02],\n",
       "           [-4.7148e-03,  1.9891e-02,  2.6470e-02, -1.1946e-02, -2.5766e-02]],\n",
       " \n",
       "          [[ 3.7226e-02, -4.6864e-03,  1.6491e-02, -2.0913e-02, -2.3070e-02],\n",
       "           [-8.2867e-03,  1.4790e-03,  2.6457e-02,  2.9052e-02, -5.6191e-03],\n",
       "           [ 4.3967e-02, -2.3808e-02,  3.0059e-03, -2.8029e-02,  7.3210e-03],\n",
       "           [ 4.0760e-02, -3.6993e-02,  1.0520e-02, -4.1100e-02, -1.0002e-02],\n",
       "           [-2.1018e-02,  7.5389e-05, -3.1036e-02,  1.1358e-02, -4.2198e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.6860e-02,  4.3296e-02, -2.9924e-02, -2.2038e-02,  3.9939e-02],\n",
       "           [-1.6479e-04,  2.6521e-02, -4.1895e-02, -1.6461e-02,  2.1999e-02],\n",
       "           [ 2.4204e-02, -4.0806e-02,  1.2538e-02, -1.7711e-02, -1.8831e-02],\n",
       "           [-2.3755e-02, -3.8213e-02, -3.2695e-02,  1.6648e-03,  3.4991e-03],\n",
       "           [ 3.1444e-02,  3.7416e-02,  1.6329e-02,  3.0608e-02,  1.7644e-03]],\n",
       " \n",
       "          [[-3.3217e-03,  2.5235e-02, -2.7866e-02, -7.5922e-03, -4.0736e-02],\n",
       "           [-1.9007e-02, -1.6597e-02, -1.8026e-02,  2.3961e-03, -7.8132e-03],\n",
       "           [ 1.0330e-02, -6.3559e-03,  4.3237e-02, -1.3790e-03,  3.5512e-02],\n",
       "           [ 1.4678e-02,  1.7101e-02, -1.1905e-02, -1.9463e-02, -1.1343e-03],\n",
       "           [ 5.5411e-03,  1.5034e-02,  1.6653e-03, -1.2524e-02,  1.2331e-02]],\n",
       " \n",
       "          [[ 2.8504e-02, -3.1897e-02, -3.7202e-02,  9.9681e-03, -4.2902e-02],\n",
       "           [-3.0383e-02,  7.5720e-03,  1.7932e-02, -3.5764e-02,  3.0141e-02],\n",
       "           [-1.8116e-02,  3.6353e-02, -1.5669e-03, -3.4955e-02, -1.4888e-02],\n",
       "           [-4.2772e-02,  1.5168e-02, -4.1946e-03,  4.2557e-02,  2.9092e-02],\n",
       "           [ 3.9318e-02,  3.0508e-02,  1.7710e-02,  2.5534e-02,  3.6595e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.8569e-03,  1.2490e-02,  2.4879e-02,  9.5263e-03,  1.8433e-02],\n",
       "           [ 2.0578e-02, -4.2443e-02,  7.6373e-03,  7.2688e-03, -2.6576e-02],\n",
       "           [ 4.3632e-02, -3.0228e-02,  4.1020e-02, -8.3908e-03, -3.2607e-02],\n",
       "           [-8.2766e-04,  3.1924e-03,  3.9253e-02, -2.9250e-02,  3.6418e-02],\n",
       "           [-4.2810e-02,  8.1378e-03,  4.2489e-02,  2.4880e-03,  2.0066e-02]],\n",
       " \n",
       "          [[-2.6000e-02,  1.4247e-03,  1.6608e-02,  3.2278e-04,  1.8563e-02],\n",
       "           [-3.0136e-02,  2.0395e-03, -8.4200e-03, -1.1993e-02,  6.0470e-03],\n",
       "           [-9.8894e-03,  1.9159e-02, -1.9990e-03, -1.4513e-02,  1.8235e-02],\n",
       "           [ 3.4565e-02, -2.5712e-02, -1.3841e-02,  1.1396e-02, -3.0837e-02],\n",
       "           [-3.8780e-02,  3.1891e-02, -4.3698e-02,  1.0412e-02,  3.1661e-03]],\n",
       " \n",
       "          [[ 4.4589e-02, -1.4566e-02, -2.7416e-02,  3.7511e-02,  1.6596e-02],\n",
       "           [ 4.2412e-02, -6.7992e-03,  3.4486e-02,  4.1034e-02,  3.7583e-02],\n",
       "           [-2.5062e-02,  4.4441e-02, -3.2598e-02, -6.6979e-03,  5.2466e-03],\n",
       "           [ 1.8442e-02,  3.4745e-02, -3.6171e-02,  4.2550e-02,  5.8369e-03],\n",
       "           [ 3.5509e-02,  1.4823e-03,  3.9446e-02, -3.9069e-02,  3.4865e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.8681e-03, -3.5415e-02,  3.1385e-02,  2.3164e-02, -4.3704e-03],\n",
       "           [-3.6635e-02,  1.1537e-02,  2.7416e-02, -3.9872e-02,  8.5402e-03],\n",
       "           [ 4.3852e-02,  2.6213e-02,  1.5963e-02,  4.0113e-02, -1.2390e-02],\n",
       "           [-1.7581e-02,  2.1335e-02, -2.9711e-02,  1.8029e-02,  7.2889e-03],\n",
       "           [-2.0480e-02,  2.2724e-02,  3.0269e-02, -4.0296e-02,  3.6318e-02]],\n",
       " \n",
       "          [[ 9.6237e-03,  1.2555e-02,  1.9304e-02,  2.9698e-02,  4.4058e-03],\n",
       "           [-1.1625e-02,  3.5235e-02,  4.3999e-02, -4.0645e-02, -3.8795e-02],\n",
       "           [ 2.1191e-02,  1.4028e-02,  3.4899e-02, -1.6600e-02,  4.1614e-02],\n",
       "           [ 3.2285e-04,  2.4654e-02,  8.5473e-03, -2.9382e-02,  1.7354e-02],\n",
       "           [-4.1063e-02,  3.8200e-02, -2.9949e-02,  1.9739e-02, -1.6705e-02]],\n",
       " \n",
       "          [[-7.5036e-03, -8.5428e-03,  2.2074e-02,  3.3235e-02, -1.4232e-03],\n",
       "           [-1.0180e-02, -2.4044e-02,  1.9051e-02, -3.7693e-03, -3.4101e-02],\n",
       "           [-2.3589e-02, -1.5302e-02, -2.9065e-02,  3.6827e-02, -3.3009e-02],\n",
       "           [ 8.3266e-03, -5.9225e-03, -4.2480e-02,  1.7390e-02, -6.8557e-03],\n",
       "           [ 1.9311e-02,  8.2411e-03, -3.0561e-02, -3.3001e-02,  4.0675e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.5769e-02, -2.2006e-02,  1.8492e-02,  2.1752e-03, -2.9444e-02],\n",
       "           [ 2.5364e-02,  3.5122e-02,  7.5939e-03,  1.6711e-02, -3.5594e-02],\n",
       "           [-1.9620e-02, -2.8978e-02, -5.6852e-03, -4.0963e-02,  4.3353e-02],\n",
       "           [-6.4657e-03, -3.5263e-02,  3.7701e-02, -1.8190e-02, -3.5652e-02],\n",
       "           [-3.1950e-02, -2.5484e-02, -2.7694e-02,  1.8116e-02, -4.4410e-02]],\n",
       " \n",
       "          [[-1.9224e-02, -4.6615e-03, -2.2357e-02,  4.3587e-02,  4.2777e-02],\n",
       "           [ 1.7315e-03, -1.7069e-02,  4.4664e-02, -4.2062e-02, -1.5023e-02],\n",
       "           [ 3.0747e-02,  4.3362e-02, -3.5018e-02, -1.7017e-02,  1.1826e-02],\n",
       "           [-3.8513e-02,  3.8379e-02,  2.5104e-02,  2.3172e-02, -1.6937e-02],\n",
       "           [-2.9021e-02,  3.0230e-02, -1.2947e-02,  6.3341e-03,  1.6521e-02]],\n",
       " \n",
       "          [[-2.4614e-02,  2.5555e-03, -2.8743e-02, -7.5108e-03, -7.3384e-03],\n",
       "           [-3.5093e-02,  2.9880e-02, -4.3697e-02, -2.8376e-02, -1.3230e-03],\n",
       "           [-1.2085e-02, -3.1916e-02, -4.0222e-02,  3.8643e-02, -4.2432e-03],\n",
       "           [-3.5138e-02,  8.3888e-03, -3.1715e-02,  7.2455e-03, -8.2536e-03],\n",
       "           [ 1.1149e-03,  4.1013e-02,  2.7426e-02,  1.0837e-02, -3.5197e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.7993e-02, -5.9009e-03,  3.7119e-02,  2.7321e-02, -3.9911e-02],\n",
       "           [ 2.2253e-02,  3.4042e-02, -3.2476e-02, -6.4674e-04, -7.3428e-03],\n",
       "           [-3.4272e-02, -2.0557e-02, -2.3619e-02, -3.9640e-02, -9.6261e-03],\n",
       "           [ 5.4342e-03,  4.1609e-02, -2.7552e-02,  1.0207e-02,  3.7912e-02],\n",
       "           [-3.5756e-03, -7.4741e-03,  3.6483e-02, -2.6904e-02,  1.8511e-02]],\n",
       " \n",
       "          [[ 1.1709e-02,  2.4596e-02, -1.8226e-03,  1.6039e-02, -2.4005e-02],\n",
       "           [-3.1616e-02,  3.1796e-02, -3.4134e-02, -2.0651e-02, -2.1769e-02],\n",
       "           [-3.1020e-03,  1.1832e-02,  3.3829e-02, -1.4065e-02, -2.3552e-02],\n",
       "           [ 3.3069e-02, -1.1851e-02,  3.8395e-03, -1.7372e-02,  2.6765e-02],\n",
       "           [-2.5934e-02, -1.3571e-02, -3.3791e-02, -1.6801e-02,  1.1701e-02]],\n",
       " \n",
       "          [[-2.9563e-02, -1.7863e-02,  4.3431e-02,  1.8756e-02, -3.2456e-02],\n",
       "           [ 8.3106e-04,  1.3162e-02,  1.5374e-02, -3.2519e-02,  4.3575e-02],\n",
       "           [-9.0789e-04, -3.1422e-02, -3.8833e-02, -9.4976e-03, -1.5666e-02],\n",
       "           [ 2.3927e-02,  2.7287e-02, -4.0093e-02, -3.9690e-02, -4.3468e-02],\n",
       "           [-3.4904e-02, -2.2839e-02, -4.2635e-02,  1.5528e-02,  1.1946e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.7915e-02, -2.9694e-02, -1.2296e-02,  3.8904e-02,  6.1156e-03],\n",
       "           [ 1.5228e-02, -2.0833e-02, -6.1910e-03, -1.1961e-02,  3.1561e-02],\n",
       "           [-4.3072e-02,  1.9849e-03,  5.9930e-03, -2.5031e-02,  1.2478e-02],\n",
       "           [-3.5451e-02,  5.8840e-03, -6.1001e-03, -3.4334e-02,  1.0781e-02],\n",
       "           [ 2.4597e-02, -1.0089e-02,  2.3273e-02,  4.1192e-02,  1.9123e-04]],\n",
       " \n",
       "          [[ 2.9095e-02,  3.0582e-02, -3.4621e-02, -3.9018e-02, -3.2892e-02],\n",
       "           [ 3.4753e-03, -1.9029e-02,  3.3480e-02, -4.7249e-03, -2.3141e-02],\n",
       "           [ 2.3616e-02,  4.0827e-02, -1.7059e-02, -3.0805e-02, -2.2362e-02],\n",
       "           [ 3.3496e-02,  2.2167e-02, -2.2117e-02,  8.6314e-03, -2.7605e-02],\n",
       "           [ 3.5504e-02,  2.4718e-02,  2.7024e-02, -1.4898e-02,  2.8558e-02]],\n",
       " \n",
       "          [[ 3.4106e-02,  3.4796e-02,  2.9610e-02, -1.9870e-02,  3.4652e-02],\n",
       "           [-4.1115e-02,  4.1473e-02, -2.2486e-02, -2.8333e-02, -2.9208e-02],\n",
       "           [ 1.0231e-03,  2.3919e-02,  3.1048e-02, -1.4368e-02,  3.2574e-03],\n",
       "           [-2.8307e-02,  1.3070e-03,  2.3852e-02, -2.6113e-02,  1.3827e-02],\n",
       "           [ 4.0868e-02,  2.5425e-02,  1.3784e-02,  1.2855e-02,  4.1786e-02]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0203, -0.0199,  0.0436,  0.0136, -0.0332, -0.0321,  0.0189, -0.0182,\n",
       "          0.0230,  0.0269, -0.0315, -0.0369, -0.0114, -0.0033,  0.0013, -0.0166,\n",
       "          0.0375, -0.0311, -0.0268, -0.0103,  0.0315,  0.0213,  0.0034,  0.0102,\n",
       "         -0.0261, -0.0019, -0.0111,  0.0406,  0.0235, -0.0047,  0.0098, -0.0133,\n",
       "          0.0073, -0.0285, -0.0135,  0.0403,  0.0015, -0.0064, -0.0290, -0.0074,\n",
       "          0.0226,  0.0408,  0.0153,  0.0107, -0.0344,  0.0382,  0.0250, -0.0227,\n",
       "         -0.0119, -0.0411], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0272, -0.0286, -0.0184,  ...,  0.0236, -0.0228,  0.0153],\n",
       "         [-0.0234, -0.0054, -0.0145,  ...,  0.0223, -0.0192,  0.0007],\n",
       "         [ 0.0079, -0.0050, -0.0150,  ...,  0.0159,  0.0159,  0.0237],\n",
       "         ...,\n",
       "         [-0.0106,  0.0098,  0.0071,  ...,  0.0114, -0.0208,  0.0233],\n",
       "         [-0.0002,  0.0225, -0.0062,  ..., -0.0230,  0.0125,  0.0288],\n",
       "         [-0.0125,  0.0301, -0.0215,  ..., -0.0144,  0.0352, -0.0332]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0211,  0.0016,  0.0336,  0.0114,  0.0207,  0.0058,  0.0303,  0.0134,\n",
       "          0.0246,  0.0135,  0.0223, -0.0267, -0.0181,  0.0062, -0.0272, -0.0082,\n",
       "         -0.0039, -0.0105,  0.0279,  0.0084,  0.0324,  0.0060, -0.0232,  0.0279,\n",
       "          0.0105, -0.0285,  0.0121,  0.0339,  0.0321,  0.0223, -0.0016, -0.0284,\n",
       "         -0.0175,  0.0077, -0.0122,  0.0053, -0.0201,  0.0054,  0.0176, -0.0091,\n",
       "          0.0267, -0.0209,  0.0196,  0.0324,  0.0034,  0.0098, -0.0215,  0.0245,\n",
       "          0.0139, -0.0240,  0.0234,  0.0261, -0.0308,  0.0142, -0.0108,  0.0141,\n",
       "         -0.0150, -0.0289, -0.0322,  0.0325, -0.0052, -0.0140,  0.0195, -0.0130,\n",
       "         -0.0174,  0.0096, -0.0111, -0.0138,  0.0151, -0.0159, -0.0041,  0.0258,\n",
       "          0.0132, -0.0308, -0.0058,  0.0352,  0.0229, -0.0194, -0.0341,  0.0345,\n",
       "         -0.0048, -0.0170,  0.0188, -0.0318, -0.0260, -0.0085, -0.0096,  0.0353,\n",
       "          0.0270, -0.0347, -0.0119,  0.0112, -0.0333,  0.0202,  0.0222, -0.0247,\n",
       "          0.0010, -0.0184,  0.0140,  0.0101,  0.0277, -0.0122, -0.0124,  0.0204,\n",
       "         -0.0142, -0.0032, -0.0107,  0.0049, -0.0291, -0.0224,  0.0323,  0.0233,\n",
       "         -0.0023,  0.0179,  0.0300,  0.0313, -0.0018,  0.0190,  0.0274,  0.0215,\n",
       "          0.0159,  0.0021,  0.0332, -0.0086,  0.0193,  0.0231, -0.0023,  0.0001,\n",
       "         -0.0077, -0.0336,  0.0197, -0.0319, -0.0192,  0.0248,  0.0242, -0.0029,\n",
       "         -0.0162, -0.0071, -0.0182, -0.0204, -0.0228, -0.0070, -0.0289, -0.0299,\n",
       "         -0.0181,  0.0339,  0.0344, -0.0136,  0.0332, -0.0254, -0.0036, -0.0168,\n",
       "         -0.0319, -0.0277,  0.0329,  0.0055, -0.0012,  0.0061, -0.0249, -0.0024,\n",
       "         -0.0321, -0.0201, -0.0327, -0.0127,  0.0280, -0.0002,  0.0249,  0.0230,\n",
       "         -0.0006,  0.0146, -0.0106, -0.0160,  0.0302,  0.0317,  0.0304,  0.0309,\n",
       "          0.0338,  0.0129, -0.0116, -0.0017, -0.0175,  0.0279, -0.0219,  0.0045,\n",
       "         -0.0317,  0.0023,  0.0110,  0.0219, -0.0213, -0.0167, -0.0170, -0.0232,\n",
       "          0.0114, -0.0106, -0.0163, -0.0263, -0.0249, -0.0286,  0.0241,  0.0230,\n",
       "          0.0273, -0.0035,  0.0302, -0.0261,  0.0320, -0.0251, -0.0114,  0.0113,\n",
       "         -0.0233,  0.0166, -0.0122, -0.0300,  0.0236, -0.0270, -0.0078, -0.0194,\n",
       "          0.0346, -0.0035,  0.0012, -0.0240,  0.0115, -0.0320,  0.0329,  0.0173,\n",
       "          0.0040,  0.0164, -0.0097,  0.0329, -0.0147, -0.0174, -0.0351, -0.0261,\n",
       "          0.0308,  0.0126,  0.0105, -0.0323,  0.0111,  0.0232,  0.0228, -0.0258,\n",
       "         -0.0325,  0.0065,  0.0189,  0.0157,  0.0279, -0.0216, -0.0340,  0.0299,\n",
       "          0.0202, -0.0097,  0.0197,  0.0311, -0.0120, -0.0006, -0.0259, -0.0279,\n",
       "         -0.0165, -0.0278, -0.0044, -0.0157, -0.0179,  0.0222, -0.0225,  0.0262,\n",
       "         -0.0287, -0.0032,  0.0035,  0.0052, -0.0222, -0.0143, -0.0233, -0.0134,\n",
       "          0.0261,  0.0213,  0.0141,  0.0093,  0.0007, -0.0331, -0.0131,  0.0324,\n",
       "          0.0112, -0.0069,  0.0315, -0.0278,  0.0159, -0.0276, -0.0253, -0.0188,\n",
       "          0.0026,  0.0198,  0.0029,  0.0247,  0.0324,  0.0174,  0.0024, -0.0206,\n",
       "          0.0211, -0.0253, -0.0124, -0.0019,  0.0054,  0.0124,  0.0063,  0.0032,\n",
       "          0.0247,  0.0315,  0.0329,  0.0034,  0.0346, -0.0074,  0.0135, -0.0318,\n",
       "         -0.0217, -0.0257,  0.0293, -0.0292,  0.0324, -0.0053,  0.0043, -0.0041,\n",
       "         -0.0215, -0.0166,  0.0190, -0.0071,  0.0223,  0.0266, -0.0323, -0.0222,\n",
       "         -0.0307, -0.0103,  0.0062,  0.0298, -0.0086, -0.0165, -0.0137,  0.0294,\n",
       "         -0.0009,  0.0133, -0.0322, -0.0340, -0.0309,  0.0252, -0.0158, -0.0018,\n",
       "         -0.0023, -0.0062,  0.0291, -0.0132, -0.0275, -0.0278, -0.0268,  0.0085,\n",
       "          0.0164,  0.0300, -0.0093,  0.0128, -0.0312,  0.0140, -0.0077, -0.0029,\n",
       "          0.0334, -0.0229, -0.0282, -0.0085,  0.0056,  0.0338,  0.0135, -0.0207,\n",
       "         -0.0306, -0.0203,  0.0121, -0.0064, -0.0079,  0.0254, -0.0021,  0.0098,\n",
       "          0.0158,  0.0189,  0.0113, -0.0267, -0.0201,  0.0171, -0.0305, -0.0342,\n",
       "         -0.0196, -0.0165, -0.0045, -0.0248,  0.0073, -0.0102,  0.0347, -0.0161,\n",
       "          0.0095, -0.0227,  0.0350, -0.0159, -0.0052, -0.0334,  0.0158,  0.0135,\n",
       "          0.0327,  0.0330, -0.0343,  0.0189, -0.0100,  0.0093, -0.0262,  0.0072,\n",
       "          0.0021, -0.0280, -0.0169,  0.0299,  0.0274, -0.0268, -0.0313,  0.0089,\n",
       "          0.0148,  0.0137, -0.0285,  0.0004, -0.0350, -0.0101, -0.0152, -0.0272,\n",
       "          0.0228,  0.0255, -0.0190,  0.0130,  0.0067, -0.0225, -0.0186, -0.0317,\n",
       "          0.0180, -0.0086, -0.0254, -0.0131, -0.0120, -0.0042, -0.0306,  0.0191,\n",
       "          0.0158, -0.0103,  0.0333,  0.0108,  0.0301, -0.0097,  0.0331,  0.0061,\n",
       "         -0.0195,  0.0147,  0.0180,  0.0253, -0.0103,  0.0091,  0.0001, -0.0292,\n",
       "         -0.0196, -0.0150,  0.0081, -0.0249, -0.0206,  0.0147, -0.0117,  0.0334,\n",
       "          0.0072,  0.0217,  0.0166, -0.0169,  0.0088, -0.0119, -0.0031, -0.0327,\n",
       "          0.0009, -0.0307,  0.0177, -0.0104,  0.0347,  0.0129,  0.0128,  0.0304,\n",
       "         -0.0244,  0.0181, -0.0303,  0.0161,  0.0177, -0.0279,  0.0156, -0.0342,\n",
       "         -0.0213, -0.0246, -0.0061, -0.0156,  0.0030, -0.0202,  0.0334, -0.0052,\n",
       "         -0.0183, -0.0193, -0.0070, -0.0246], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0377,  0.0204,  0.0388,  ..., -0.0094,  0.0226, -0.0135],\n",
       "         [ 0.0368,  0.0108, -0.0306,  ...,  0.0096, -0.0027,  0.0302],\n",
       "         [-0.0271,  0.0034,  0.0141,  ...,  0.0212, -0.0338, -0.0122],\n",
       "         ...,\n",
       "         [ 0.0212,  0.0193,  0.0073,  ..., -0.0019, -0.0218,  0.0394],\n",
       "         [-0.0206, -0.0284, -0.0056,  ...,  0.0049,  0.0275, -0.0143],\n",
       "         [ 0.0216,  0.0267,  0.0195,  ...,  0.0178, -0.0287, -0.0261]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0255,  0.0068, -0.0092, -0.0006,  0.0272, -0.0248,  0.0132,  0.0376,\n",
       "          0.0379,  0.0172], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 paramete 확인하는 방법\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([50, 20, 5, 5])\n",
      "torch.Size([50])\n",
      "torch.Size([500, 800])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "for i in range(8):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([20, 1, 5, 5])  <-- Conv 의 weight size\n",
    "# torch.Size([20])           <-- bias\n",
    "# torch.Size([50, 20, 5, 5]) <-- Conv 의 weight size\n",
    "# torch.Size([50])           <-- bias\n",
    "# torch.Size([500, 800])     <-- fully connected..\n",
    "# torch.Size([500])\n",
    "# torch.Size([10, 500])\n",
    "# torch.Size([10])\n",
    "\n",
    "# keras 에선 model summary 기능이 있었다\n",
    "# ↑ PyTorch 는 summary 기능이 없기 때문에 이렇게 함 들여다 볼수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.5\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n",
    "            # SGD : Stochastic Gradient Descent\n",
    "            # 1. model 의 parameter 들 입력\n",
    "            # 2. lr (learning rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**momentum** 이란?\n",
    "\n",
    "![](https://media.vlpt.us/images/reversesky/post/780406cf-3482-4e67-b09a-a955d5be1d1c/image.png)\n",
    "\n",
    "loss의 미분값이 파라미터의 값이 0에 존재한다고 생각해보자. 위의 사진을 보면, 우리가 구하려는 전역 최소값에 가기전, 지역 최소값에 도달하면 미분값이 0이 되면서 더 이상 움직이지 않는다. 그래서 sgd에 약간의 변형을 준 것이 SGD+Momentum이라는 개념이다.\n",
    "\n",
    "\n",
    "sgd에다가 이전의 이동값을 고려하도록 설계하여 momentum. 즉 관성을 주었다. 혹여 지역 최소값에 도달하더라도 앞으로 나아가서 지역 최소값을 탈출할 수 있도록 설정해준다.\n",
    "\n",
    "\n",
    "※참고로 Adam 은 sgd + Momentum + RMSProp 을 같이 사용함 (일반적으로 추천)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Training\n",
    "\n",
    "- 학습하기 전에 Model이 Train할 수 있도록 Train Mode로 변환\n",
    "    - Convolution 또는 Linear 뿐만 아니라, DropOut과 추후에 배우게 될 Batch Normalization과 같이 parameter를 가진 Layer들도 학습하기 위해 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()   # 학습하기 전에 Train mode 로 전환해야 함\n",
    "\n",
    "# train mode 했다가, evaluation mode 했다가,  다시 학습할때는 train mode 로 돌아와야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 넣기 위한 첫 Batch 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(train_loader))  # 첫 batch 만 꺼내와 보기 (현재 batch size 는 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추출한 Batch 데이터를 device 에 compile \n",
    "    - device 는 'cpu' 또는 'gpu'.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = data.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, target.shape  # shape 가 바뀌는 건 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 Optimizer 를 설정해 주었다.\n",
    "# 학습하기 전에 Optimizer 를 clear 해 주어야 한다\n",
    "# zero_grad()  -> 새로운 최적값을 찾기 위해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()  # Optimizer clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 준비한 데이터를 model 에 input 으로 넣어 output 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 이 나왔으니\n",
    "# 얼마나 틀렸는지 (Loss) 를 봐야 한다\n",
    "\n",
    "# output 을 target 과 비교하여 얼마나 틀렸는지를 loss function  에 넣어본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 에서 예측한 결과를 Loss Function 에 넣음\n",
    "- 이번 예제에선 Negative Log-Likelihood Loss 라는 Loss Function 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2922, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.nll_loss(output, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back Propagation 을 통해 gradients 를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4b5865e23e9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 기울기(gradient) 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "loss.backward()   # 기울기(gradient) 계산\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기가 계산되었다고 끝이 아니다 -> Optimizer 에 업데이트 해주어야 한다. (Parameter Update)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상이 '학습' 의 \"1 스텝\"입니다\n",
    "- train 모드 변환\n",
    "- 데이터 넣어주고\n",
    "- 기울기 clear\n",
    "- model 에 데이터 넣고\n",
    "- loss 계산하고\n",
    "- back propagation 하여 gradient 계산하고\n",
    "- parameter 업데이트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "위의 최적화 과정을 반복하여 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypter paramete 설정\n",
    "epochs = 1\n",
    "log_interval = 100  # 로그를 확인하기 위해 몇 스텝마다 볼지 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295366\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.220284\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.136016\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.953834\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.498464\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.254101\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.842483\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.706224\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.586595\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.612400\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    # 1. train 모드 변환\n",
    "    model.train()\n",
    "    \n",
    "    # for 한번 할때마다 한번 학습 이루어짐\n",
    "    # 2. 데이터 넣어주기\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader): # 매 iteration 마다 batch index 받아온다.\n",
    "        data, target = data.to(device), target.to(device)    # 데이터를 device 에 compile\n",
    "        # 3. 기울기 clear\n",
    "        optimizer.zero_grad()\n",
    "        # 4. model 에 데이터 넣기\n",
    "        output = model(data)\n",
    "        # 5. loss 계산\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # 6. back propagation 하여 gradient 계산\n",
    "        loss.backward()\n",
    "        # 7. parameter 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:  # 중간중간에 로그 확인\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
